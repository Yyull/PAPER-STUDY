<html><head><title>Going Deeper with Convolutions</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(0, 0, 0, 0.06); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body class="pdf ko-KR" lang="ko-KR"><article id="af6dbf74-d34e-4c22-a294-17ad2f7e7362" class="page serif"><header><div class="page-header-icon undefined"><img class="icon" src="https://img.icons8.com/ios/250/000000/notepad.png"/></div><h1 class="page-title">Going Deeper with Convolutions</h1><p class="page-description"></p><table class="properties"><tbody><tr class="property-row property-row-file"><th><span class="icon property-icon"><svg role="graphics-symbol" viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0" class="typesFile"><path d="M12.5938 8.44043L7.54883 13.4854C6.26367 14.7773 4.54102 14.6475 3.44043 13.54C2.32617 12.4326 2.20312 10.7168 3.49512 9.4248L10.3926 2.52734C11.1445 1.77539 12.2725 1.63867 13.0039 2.37695C13.7422 3.11523 13.6055 4.22949 12.8535 4.98828L6.08594 11.7627C5.77832 12.0703 5.41602 11.9814 5.2041 11.7764C4.99902 11.5576 4.91016 11.2021 5.21777 10.8877L9.93457 6.1709C10.1738 5.93164 10.1875 5.58301 9.96191 5.35059C9.72949 5.13184 9.38086 5.13867 9.1416 5.37793L4.39746 10.1152C3.67285 10.8535 3.7002 11.9746 4.34961 12.624C5.05371 13.3281 6.12012 13.3145 6.8584 12.5762L13.6602 5.77441C14.9795 4.45508 14.9316 2.71875 13.7764 1.55664C12.6416 0.428711 10.8779 0.34668 9.55176 1.66602L2.61328 8.61816C0.883789 10.3477 1 12.8291 2.57227 14.4014C4.14453 15.9668 6.63281 16.0898 8.3623 14.3672L13.4414 9.28809C13.6738 9.0625 13.667 8.64551 13.4346 8.41992C13.209 8.18066 12.833 8.21484 12.5938 8.44043Z"></path></svg></span>URL</th><td><span style="margin-right:6px"><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F366ee882-adac-4ea9-b8e2-1e685459a8a9%2F4890ec1b-a65c-44a3-8484-e229c7cf1eee%2F1409.4842v1.pdf?table=block&amp;id=af6dbf74-d34e-4c22-a294-17ad2f7e7362&amp;spaceId=366ee882-adac-4ea9-b8e2-1e685459a8a9&amp;userId=f60ec15c-1827-44c1-b028-caf44280c5a8&amp;cache=v2">1409.4842v1.pdf</a></span><span style="margin-right:6px"><a href="https://arxiv.org/abs/1409.4842">https://arxiv.org/abs/1409.4842</a></span></td></tr><tr class="property-row property-row-date"><th><span class="icon property-icon"><svg role="graphics-symbol" viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0" class="typesDate"><path d="M3.29688 14.4561H12.7031C14.1797 14.4561 14.9453 13.6904 14.9453 12.2344V3.91504C14.9453 2.45215 14.1797 1.69336 12.7031 1.69336H3.29688C1.82031 1.69336 1.05469 2.45215 1.05469 3.91504V12.2344C1.05469 13.6973 1.82031 14.4561 3.29688 14.4561ZM3.27637 13.1162C2.70898 13.1162 2.39453 12.8154 2.39453 12.2207V5.9043C2.39453 5.30273 2.70898 5.00879 3.27637 5.00879H12.71C13.2842 5.00879 13.6055 5.30273 13.6055 5.9043V12.2207C13.6055 12.8154 13.2842 13.1162 12.71 13.1162H3.27637ZM6.68066 7.38086H7.08398C7.33008 7.38086 7.41211 7.30566 7.41211 7.05957V6.66309C7.41211 6.41699 7.33008 6.3418 7.08398 6.3418H6.68066C6.44141 6.3418 6.35938 6.41699 6.35938 6.66309V7.05957C6.35938 7.30566 6.44141 7.38086 6.68066 7.38086ZM8.92285 7.38086H9.31934C9.56543 7.38086 9.64746 7.30566 9.64746 7.05957V6.66309C9.64746 6.41699 9.56543 6.3418 9.31934 6.3418H8.92285C8.67676 6.3418 8.59473 6.41699 8.59473 6.66309V7.05957C8.59473 7.30566 8.67676 7.38086 8.92285 7.38086ZM11.1582 7.38086H11.5547C11.8008 7.38086 11.8828 7.30566 11.8828 7.05957V6.66309C11.8828 6.41699 11.8008 6.3418 11.5547 6.3418H11.1582C10.9121 6.3418 10.8301 6.41699 10.8301 6.66309V7.05957C10.8301 7.30566 10.9121 7.38086 11.1582 7.38086ZM4.44531 9.58203H4.84863C5.09473 9.58203 5.17676 9.50684 5.17676 9.26074V8.86426C5.17676 8.61816 5.09473 8.54297 4.84863 8.54297H4.44531C4.20605 8.54297 4.12402 8.61816 4.12402 8.86426V9.26074C4.12402 9.50684 4.20605 9.58203 4.44531 9.58203ZM6.68066 9.58203H7.08398C7.33008 9.58203 7.41211 9.50684 7.41211 9.26074V8.86426C7.41211 8.61816 7.33008 8.54297 7.08398 8.54297H6.68066C6.44141 8.54297 6.35938 8.61816 6.35938 8.86426V9.26074C6.35938 9.50684 6.44141 9.58203 6.68066 9.58203ZM8.92285 9.58203H9.31934C9.56543 9.58203 9.64746 9.50684 9.64746 9.26074V8.86426C9.64746 8.61816 9.56543 8.54297 9.31934 8.54297H8.92285C8.67676 8.54297 8.59473 8.61816 8.59473 8.86426V9.26074C8.59473 9.50684 8.67676 9.58203 8.92285 9.58203ZM11.1582 9.58203H11.5547C11.8008 9.58203 11.8828 9.50684 11.8828 9.26074V8.86426C11.8828 8.61816 11.8008 8.54297 11.5547 8.54297H11.1582C10.9121 8.54297 10.8301 8.61816 10.8301 8.86426V9.26074C10.8301 9.50684 10.9121 9.58203 11.1582 9.58203ZM4.44531 11.7832H4.84863C5.09473 11.7832 5.17676 11.708 5.17676 11.4619V11.0654C5.17676 10.8193 5.09473 10.7441 4.84863 10.7441H4.44531C4.20605 10.7441 4.12402 10.8193 4.12402 11.0654V11.4619C4.12402 11.708 4.20605 11.7832 4.44531 11.7832ZM6.68066 11.7832H7.08398C7.33008 11.7832 7.41211 11.708 7.41211 11.4619V11.0654C7.41211 10.8193 7.33008 10.7441 7.08398 10.7441H6.68066C6.44141 10.7441 6.35938 10.8193 6.35938 11.0654V11.4619C6.35938 11.708 6.44141 11.7832 6.68066 11.7832ZM8.92285 11.7832H9.31934C9.56543 11.7832 9.64746 11.708 9.64746 11.4619V11.0654C9.64746 10.8193 9.56543 10.7441 9.31934 10.7441H8.92285C8.67676 10.7441 8.59473 10.8193 8.59473 11.0654V11.4619C8.59473 11.708 8.67676 11.7832 8.92285 11.7832Z"></path></svg></span>DATE</th><td><time>@2024년 7월 21일</time></td></tr><tr class="property-row property-row-status"><th><span class="icon property-icon"><svg role="graphics-symbol" viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0" class="typesStatus"><path d="M8.75488 1.02344C8.75488 0.613281 8.41309 0.264648 8.00293 0.264648C7.59277 0.264648 7.25098 0.613281 7.25098 1.02344V3.11523C7.25098 3.51855 7.59277 3.86719 8.00293 3.86719C8.41309 3.86719 8.75488 3.51855 8.75488 3.11523V1.02344ZM3.91504 5.0293C4.20215 5.31641 4.69434 5.32324 4.97461 5.03613C5.26855 4.74902 5.26855 4.25684 4.98145 3.96973L3.53906 2.52051C3.25195 2.2334 2.7666 2.21973 2.47949 2.50684C2.19238 2.79395 2.18555 3.28613 2.47266 3.57324L3.91504 5.0293ZM10.9629 4.01758C10.6826 4.30469 10.6826 4.79688 10.9697 5.08398C11.2568 5.37109 11.749 5.36426 12.0361 5.07715L13.4854 3.62793C13.7725 3.34082 13.7725 2.84863 13.4785 2.55469C13.1982 2.27441 12.7061 2.27441 12.4189 2.56152L10.9629 4.01758ZM15.0234 8.78906C15.4336 8.78906 15.7822 8.44727 15.7822 8.03711C15.7822 7.62695 15.4336 7.28516 15.0234 7.28516H12.9385C12.5283 7.28516 12.1797 7.62695 12.1797 8.03711C12.1797 8.44727 12.5283 8.78906 12.9385 8.78906H15.0234ZM0.975586 7.28516C0.56543 7.28516 0.223633 7.62695 0.223633 8.03711C0.223633 8.44727 0.56543 8.78906 0.975586 8.78906H3.07422C3.48438 8.78906 3.83301 8.44727 3.83301 8.03711C3.83301 7.62695 3.48438 7.28516 3.07422 7.28516H0.975586ZM12.0361 10.9902C11.749 10.71 11.2568 10.71 10.9629 10.9971C10.6826 11.2842 10.6826 11.7764 10.9697 12.0635L12.4258 13.5127C12.7129 13.7998 13.2051 13.793 13.4922 13.5059C13.7793 13.2256 13.7725 12.7266 13.4854 12.4395L12.0361 10.9902ZM2.52051 12.4395C2.22656 12.7266 2.22656 13.2188 2.50684 13.5059C2.79395 13.793 3.28613 13.7998 3.57324 13.5127L5.02246 12.0703C5.31641 11.7832 5.31641 11.291 5.03613 11.0039C4.74902 10.7168 4.25684 10.71 3.96973 10.9971L2.52051 12.4395ZM8.75488 12.9658C8.75488 12.5557 8.41309 12.207 8.00293 12.207C7.59277 12.207 7.25098 12.5557 7.25098 12.9658V15.0576C7.25098 15.4609 7.59277 15.8096 8.00293 15.8096C8.41309 15.8096 8.75488 15.4609 8.75488 15.0576V12.9658Z"></path></svg></span>Status</th><td><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Done</span></td></tr></tbody></table></header><div class="page-body"><h3 id="a3bcc16e-d723-4437-88fb-e13348fddc9f" class="">Abstract</h3><ul id="327bda5d-8546-4ab0-9182-381323d848d9" class="bulleted-list"><li style="list-style-type:disc"><strong>“Inception”: Deep convolutional neural network architecture</strong><ul id="01728dbd-ac9f-4f8b-bf5c-34679be704b5" class="bulleted-list"><li style="list-style-type:circle">Image classification과 detection에서 성능을 크게 향상시킴, ILSVRC14에서 입증됨.</li></ul><ul id="66d42f43-c47b-42eb-b14f-862ddf8953ba" class="bulleted-list"><li style="list-style-type:circle">컴퓨터 자원의 효율적 활용, 네트워크의 깊이와 너비를 증가시키면서 계산 예산을 늘리지X</li></ul><ul id="fdd0b346-9fe5-485b-afce-854875500c9a" class="bulleted-list"><li style="list-style-type:circle">22개의 레이어로 이루어진 GoogleNet 모델</li></ul><ul id="b4d2e5fc-f682-4322-ba8d-ed4eb850f67e" class="bulleted-list"><li style="list-style-type:circle">Hebbian principle과 multi-scale processing에 기반하여 계산 효율성을 유지하면서 더 나은 quality를 제공함.</li></ul></li></ul><p id="c71a6619-8f99-4c77-b96b-4c9a005467ed" class="">
</p><h3 id="ef3adf99-9344-4b84-8237-bb466eeba60b" class="">Introduction</h3><ul id="725a1225-f5e0-4043-b8b7-723c5fbd03a1" class="bulleted-list"><li style="list-style-type:disc">GoogLeNet은 2년 전 우승한 AlexNet 아키텍처보다 12배 적은 파라미터를 사용하면서 더 높은 정확도를 가짐.</li></ul><ul id="a234d880-2d7b-4b58-a96d-91b31a84ce34" class="bulleted-list"><li style="list-style-type:disc">모바일과 임베디드 컴퓨팅의 발전으로 알고리즘의 효율성, 특히 전력 및 메모리 사용의 중요성이 커졌다.</li></ul><ul id="a6b843b0-6d6f-4867-b62f-e0e3a47ffd31" class="bulleted-list"><li style="list-style-type:disc">이 논문에서는 Inception이라는 효율적인 심층 신경망 아키텍처에 대해 다루며,  이 아키텍처의 이점은 ILSVRC 2014 분류 및 탐지 과제에서 실험적으로 검증되었고, 최신 기술을 훨씬 능가하는 성능을 보인다.</li></ul><p id="6de1c904-79ab-47d5-97a2-1f941954c9f0" class="">
</p><h3 id="7b2b8450-8b6b-40de-9c15-ed6771b0d07d" class="">Related Work</h3><ul id="0dec286a-1139-47a6-a6e6-167ac79aee3e" class="bulleted-list"><li style="list-style-type:disc">LeNet-5를 시작으로 CNN은 일반적으로 여러 convolutional layer를 쌓고, 이후  fully-connected layer를 사용하는 표준 구조를 가짐<ul id="de163146-dd05-4017-8d1a-8c80240a9699" class="bulleted-list"><li style="list-style-type:circle"> MNIST, CIFAR, 특히 ImageNet 분류 과제에서 가장 좋은 결과를 얻었다.</li></ul></li></ul><ul id="41d19985-1eaf-4e0b-afb1-01c8ecbf42d2" class="bulleted-list"><li style="list-style-type:disc">최근에는 더 많은 레이어와 레이어 크기를 증가시키고 dropout을 사용하여 과적합 문제를 해결하는 경향이 있다.</li></ul><ul id="55a4a185-54dc-428c-9393-2d26ee4223ed" class="bulleted-list"><li style="list-style-type:disc">Network-in-Network 접근법은 1×1 convolutional layer를 추가하고 그 뒤에 ReLu activation을 추가함 → 신경망의 표현력을 높이는 방법 <ul id="2ebf6809-6785-4e72-ba5c-0692ada97dac" class="bulleted-list"><li style="list-style-type:circle">이 접근법은 주로 차원 축소 모듈로 사용되어 네트워크의 깊이와 너비를 성능 저하 없이 증가시킴.</li></ul></li></ul><ul id="e21cc030-8a7b-463f-b682-8a3c8e7523d8" class="bulleted-list"><li style="list-style-type:disc">이 논문의 GoogleNet에서도  1×1 convolutional layer를 사용하고 비슷한 pipeline을 채택했으며, 나은 분류를 위해 앙상블 접근법 등 여러 개선점을 탐구했음.</li></ul><p id="c46b4340-3f4c-4211-bf04-dad0ec4b2cbe" class="">
</p><h3 id="86f26e97-d6ed-4c50-8654-cafd3e40df85" class=""><strong>Motivation and High Level Considerations</strong></h3><ul id="733931b6-d28e-46ea-9990-4a0092167fdd" class="bulleted-list"><li style="list-style-type:disc">Deep neural network의 성능을 개선하는 방법은 network의 depth(level의 수 증가)와 width(각 level의 유닛 수 증가)를 증가시키는 것 <ul id="2641581e-44f1-4ab1-9a63-e3b1f99fb6d6" class="bulleted-list"><li style="list-style-type:circle">overfitting과 gradient vanishing이 발생할 수 있음, 컴퓨터 리소스 사용이 급격히 증가함</li></ul><figure id="217d834f-91f7-42dd-8124-10d7f08fd910" class="image"><img style="width:613px" src="https://prod-files-secure.s3.us-west-2.amazonaws.com/366ee882-adac-4ea9-b8e2-1e685459a8a9/50f1bfbe-676d-4f7c-b4ac-d71cf4c7f765/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45HZZMZUHI%2F20240728%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20240728T122023Z&amp;X-Amz-Expires=86400&amp;X-Amz-Signature=fba80962472b2ea4425c6f7a788b5053e9e5fd2504249a7770907fbe1c641ba5&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject"/></figure></li></ul><ul id="3fb1f463-5dd8-4fe4-9904-504ae4d68ade" class="bulleted-list"><li style="list-style-type:disc">위의 2가지 문제를 해결하는 방법: dense한 fully-connected 구조에서 sparesely connected 구조로 바꾸는 것</li></ul><ul id="04e73c20-c836-45c5-9cee-ae5be6d33f79" class="bulleted-list"><li style="list-style-type:disc">하지만 오늘날의 컴퓨팅 환경은 균일하지 않은 sparse data 구조를 다룰 때 매우 비효율적임.</li></ul><ul id="1c5a5093-874c-42f4-bdc9-e3960ec24101" class="bulleted-list"><li style="list-style-type:disc">다른 연구에서 sparse matrix를 clustering하여 상대적으로 dense한 submatric를 만드는 것을 제안하였고 좋은 성능을 보였음.</li></ul><ul id="50be9d3d-593b-4c61-ba52-5e0a31944f25" class="bulleted-list"><li style="list-style-type:disc">Inception 구조는 유사 sparse 구조를 시험하기 위해 시작됨, 여러 번의 반복을 거쳐 개선된 성능을 보였으며 localization 및 object detection 분야에서 특히 좋은 성능을 보였다고 함.</li></ul><p id="a43f5cd6-57be-4fac-98ec-d049163c9450" class="">
</p><h3 id="04d01afb-0874-4598-9bf6-5e6c81f504db" class=""><strong>Architectural Details</strong></h3><ul id="d1fb9703-4a37-4046-bffb-2162232808db" class="bulleted-list"><li style="list-style-type:disc">Inception 구조의 주요 아이디어는 CNN에서 각 요소를 최적의 local sparse structure로 근사화하고, 이를 dense component로 바꾸는 방법을 찾는 것 <ul id="7ce1b069-cc93-433a-b5b5-657f6ec917fd" class="bulleted-list"><li style="list-style-type:circle">최적의 local 구성 요소를 찾고 공간적으로 반복 → sparse matrix를 clustering하여 상대적으로  dense한 submatrix를 만드는 것</li></ul></li></ul><ul id="d356ae99-365c-4a21-bb31-b95efd76ffbf" class="bulleted-list"><li style="list-style-type:disc">이전 layer의 각 유닛이 입력 이미지의 특정 부분에 해당한다고 가정함<ul id="34d6c614-a772-4334-a471-ecb0405b975c" class="bulleted-list"><li style="list-style-type:circle">입력 이미지와 가까운 낮은 layer에서는 특정 부분에 correlated unit이 집중돼있음 </li></ul><ul id="dc716f77-fb90-41a7-9dab-704eed46befa" class="bulleted-list"><li style="list-style-type:circle">단일 지역에 많은 cluster들이 집중된다는 뜻이므로 1x1 convolution으로 처리할 수 있음</li></ul></li></ul><figure id="d22f2ae3-4749-47e8-99d8-22200a0de24b" class="image"><img style="width:776px" src="https://prod-files-secure.s3.us-west-2.amazonaws.com/366ee882-adac-4ea9-b8e2-1e685459a8a9/c0d8e67c-c704-423f-aa2c-89c341e599eb/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45HZZMZUHI%2F20240728%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20240728T122023Z&amp;X-Amz-Expires=86400&amp;X-Amz-Signature=27776c5d021670f9994879e9428026c76e16afa900efb55627587a76e72d0536&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject"/></figure><ul id="da7344d4-45b2-47c6-8932-9fd8a529b84c" class="bulleted-list"><li style="list-style-type:disc">몇몇 위치에서는 더 넓은 영역의 convolutional filter가 있어야 correlated unit의 비율을 높일 수 있는 상황이 나타날 수 있음 <ul id="bd08e8aa-88ba-4978-82cc-c76c8f4cea6e" class="bulleted-list"><li style="list-style-type:circle">feature map을 효과적으로 추출할 수 있도록 1x1, 3x3, 5x5 convolution 연산을 병렬적으로 수행</li></ul><ul id="8225dacd-e58b-46b3-86ec-12bc132b0c8e" class="bulleted-list"><li style="list-style-type:circle">CNN에서 pooling layer의 성능은 입증되었으므로 높이와 폭을 맞추기 위해 padding 추가</li></ul></li></ul><figure id="38d71184-8f9a-4ce9-a464-77a4b3f6aea4" class="image"><img style="width:802px" src="https://prod-files-secure.s3.us-west-2.amazonaws.com/366ee882-adac-4ea9-b8e2-1e685459a8a9/2af5f98f-7a52-41aa-a1cc-eaef1e1918c9/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45HZZMZUHI%2F20240728%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20240728T122023Z&amp;X-Amz-Expires=86400&amp;X-Amz-Signature=b252d4fc5dc60ca5982babdffc1f6fac21ef303845c4d7439fb5f53f107d3071&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject"/></figure><ul id="1afe1b80-f1f2-47d2-97e9-fa51d5b2c998" class="bulleted-list"><li style="list-style-type:disc">3x3, 5x5 convolutional filter도 사용할 경우, 연산량이 많아짐 <ul id="cc42861d-e3f4-47da-9a94-22dc6df60e08" class="bulleted-list"><li style="list-style-type:circle">입력 feature map의 크기가 크거나 5x5 convolutional filte의 수가 많아지면 연산량이 더욱 증가하는 문제 발생</li></ul></li></ul><ul id="6507915f-b3e6-49e2-a71e-7f04bc418ec8" class="bulleted-list"><li style="list-style-type:disc">이 문제를 해결하기 위해 1x1 convolutional filter를 이용하여 차원을 축소함 <ul id="ddb63f9e-80c3-48e1-aba0-a925d3fc3b28" class="bulleted-list"><li style="list-style-type:circle">3x3과 5x5 앞에 1x1을 두어 차원을 줄이면 여러 scale을 확보하면서 연산량을 줄일 수 있음</li></ul><ul id="349cc4c6-2716-4e8b-9dba-2da5f354ffd9" class="bulleted-list"><li style="list-style-type:circle">convolution 연산 이후에 추가되는 ReLU를 통해 비선형적 특징을 추가할 수 있음.</li></ul></li></ul><ul id="35cc6117-9eca-4658-8f0f-7ab4b506c85f" class="bulleted-list"><li style="list-style-type:disc">효율적인 메모리 사용을 위해 낮은 layer에서는 기본적인 CNN 모델을 적용하고, 높은 layer에서는 Inception module을 사용하는 것이 좋다고 함.<ul id="c42c8079-0d8b-4c6e-9565-616f3e1226a8" class="bulleted-list"><li style="list-style-type:circle">Inception module을 사용하면 과도한 연산량 문제업이 각 단계에서 유닛 수를 상당히 증가시킬 수 있다 → 차원 축소를 통해 다음 layer의 input 수를 조절할 수 있기 떄문</li></ul><ul id="c6bbb5bf-42b6-4770-b1d5-efb8fc4dc8ac" class="bulleted-list"><li style="list-style-type:circle">visual 정보가 다양한 scale로 처리되고, 다음 layer는 동시에 서로 다른 layer에서 특징을 추출할 수 있다 → 1x1, 3x3, 5x5 convolution 연산을 통해 다양한 특징을 추출할 수 있기 때문</li></ul><p id="39c47242-e850-42f2-8da8-02b6a3320b24" class="">
</p></li></ul><h3 id="91d8ceb4-2e98-4183-a87f-f1cb44164cf7" class="">GoogLeNet</h3><ul id="94476307-31c9-4505-b8ca-9896a090f816" class="bulleted-list"><li style="list-style-type:disc">Inception module이 적용된 전체 GoogleNet의 구조</li></ul><ul id="98660b32-5998-40ee-8657-78c005934b69" class="bulleted-list"><li style="list-style-type:disc">GoogleNet이라는 이름은 LeNet으로부터 유래함</li></ul><ul id="b0de50cc-81a7-4f2c-a35d-9936cc2a6237" class="bulleted-list"><li style="list-style-type:disc">네트워크 깊이는 22 layers (pooling까지 포함하면 27 layers)</li></ul><ul id="c3098878-449f-456e-be91-614ad8441b7f" class="bulleted-list"><li style="list-style-type:disc">Inception module 내부를 포함한 모든 convolutinal layer에는 ReLU가 적용돼있음 + receptive field의 크기는 224x224로 RGB 컬러 채널을 가지며, mean subtraction을 적용한다.</li></ul><figure id="ee265588-dd68-41de-b97e-7e121f3973bd" class="image"><img style="width:768px" src="https://prod-files-secure.s3.us-west-2.amazonaws.com/366ee882-adac-4ea9-b8e2-1e685459a8a9/da3fd265-cffb-49d0-8767-79c63f1f0401/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45HZZMZUHI%2F20240728%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20240728T122023Z&amp;X-Amz-Expires=86400&amp;X-Amz-Signature=c7fe2e00312e51907f7be0940c1c5151e0f979bec79bad1ef3fbdce4f6e97863&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject"/><figcaption>Inception 구조에 대한 GoogleNet 성체</figcaption></figure><ul id="ce4dd1f9-3332-481d-8a66-d49d7df26f7b" class="bulleted-list"><li style="list-style-type:disc">#3x3 reduce와 #5x5 reduce는 3x3과 5x5 convolutional layer 앞에 사용되는 1x1 필터의 채널 수를 의미</li></ul><ul id="09a5aff7-d021-4547-90e5-e9ea0d047cd2" class="bulleted-list"><li style="list-style-type:disc">pool proj는 max pooling layer 뒤에 오는 1x1 필터의 채널 수 의미</li></ul><ul id="f65a29cc-49b8-40a8-915c-1574df5432d7" class="bulleted-list"><li style="list-style-type:disc">모든 reduction 및 projection layer에 ReLU가 사용됨</li></ul><p id="2ed7f8f6-ff64-4816-a8c8-3debdfdf0903" class="">
</p><ol type="1" id="6935fcae-69a8-4205-89d3-40a5b23df53d" class="numbered-list" start="1"><li>입력 이미지와 가까운 낮은 레이어가 위치해 있는 부분</li></ol><figure id="55074595-218a-4155-9e66-39e3c56407d0" class="image"><img style="width:480px" src="https://prod-files-secure.s3.us-west-2.amazonaws.com/366ee882-adac-4ea9-b8e2-1e685459a8a9/53fe49a3-40b5-4fdd-9101-b7035ea2f98b/86fa8b16-d29d-4ad7-8177-98050498903f.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45HZZMZUHI%2F20240728%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20240728T122023Z&amp;X-Amz-Expires=86400&amp;X-Amz-Signature=1bb46f2e0b5032f21325e3c1567eed66c16aab79ce729f2736bee7ceaf05798f&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject"/></figure><ul id="26442264-cadf-491e-8d76-7f0c7ef993af" class="bulleted-list"><li style="list-style-type:disc">효율적인 메모리 사용을 위해 낮은 layer에서는 기본적인 CNN 모델을 적용</li></ul><p id="b300b037-e7af-4aaa-bbe3-f7943c158c93" class="">
</p><ol type="1" id="69ce72bc-163d-4d9b-b613-e47f82c2eb41" class="numbered-list" start="2"><li>Inception module</li></ol><figure id="eb68fe36-dd9f-46f6-b140-4ef241bb1104" class="image"><img style="width:384px" src="https://prod-files-secure.s3.us-west-2.amazonaws.com/366ee882-adac-4ea9-b8e2-1e685459a8a9/37e3190d-926e-4ec9-9921-cbd07e45d070/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45HZZMZUHI%2F20240728%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20240728T122023Z&amp;X-Amz-Expires=86400&amp;X-Amz-Signature=beb0a74b11169cbc0aff8fa9971fff48166fe3d386df9953a9492aff22588ee3&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject"/></figure><ul id="0968e08e-e905-49aa-8649-68edff9a1fe6" class="bulleted-list"><li style="list-style-type:disc">다양한 특징을 추출하기 위해 1x1, 3x3, 5x5 convolutional layer가 병렬적으로 연산을 수행</li></ul><ul id="bc5ff051-25c7-4b2d-b701-ff073066f724" class="bulleted-list"><li style="list-style-type:disc">차원을 축소해서 연산량을 줄이기 위해 1x1 convolutional layer가 적용돼있음</li></ul><p id="09c412eb-bd76-47ed-bd5f-d089511d31d1" class="">
</p><ol type="1" id="71f310ea-67a3-4cd4-8908-ba1d9c5f8af8" class="numbered-list" start="3"><li>Auxiliary classifier가 적용된 부분 (softmax)</li></ol><figure id="4fa8530a-b858-4198-9dd2-85e5c8da3af5" class="image"><img style="width:432px" src="https://prod-files-secure.s3.us-west-2.amazonaws.com/366ee882-adac-4ea9-b8e2-1e685459a8a9/5972dcad-92b0-4f5d-8e1f-2d881afe8ea8/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45HZZMZUHI%2F20240728%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20240728T122023Z&amp;X-Amz-Expires=86400&amp;X-Amz-Signature=7d5bc1d0431da588026cafc09e259e519978a0e8162bfb759d5daa41fd04d4ed&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject"/></figure><ul id="052c42c4-3f9a-46c3-81b2-93ae7af2498c" class="bulleted-list"><li style="list-style-type:disc">모델의 깊이가 매우 깊으면 기울이가 0으로 수렴하는 gradient vanishing 문제가 발생할 수 있음</li></ul><ul id="662e83cb-206e-4276-8571-bcb2bebbd8b8" class="bulleted-list"><li style="list-style-type:disc">중간 layer에 auxiliary classifier를 추가하여, 중간중간 결과를 출력해 추가적인 역전파를 일으켜 gradient가 전달될 수 있게 하면서 정규화 효과가 나타나도록 함</li></ul><ul id="713ab866-57bf-4504-8b4d-b9675b1caf8c" class="bulleted-list"><li style="list-style-type:disc">지나치게 영향을 주는 것을 막기 위해 auxiliary classifier의 loss에 0.3을 곱하고, 실제 테스트 시에는 auxiliary classifier를 제거 후, 제일 끝단의 softmax만 사용함.</li></ul><p id="0a0e895b-7882-48ef-8a88-b69b1922fafd" class="">
</p><ol type="1" id="1162eea5-8efe-48c2-9b8c-b2a589698023" class="numbered-list" start="4"><li>모델의 끝 부분</li></ol><figure id="149fca83-ce40-475a-b8dd-925cfb4268d4" class="image"><img style="width:144px" src="https://prod-files-secure.s3.us-west-2.amazonaws.com/366ee882-adac-4ea9-b8e2-1e685459a8a9/efba03a5-8bce-451b-b62c-be9d8cbdeb01/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45HZZMZUHI%2F20240728%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20240728T122023Z&amp;X-Amz-Expires=86400&amp;X-Amz-Signature=870ad06c0427165c1adae8a3e4f5ff3382071118a975d0ff8d4169641c26a5b3&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject"/></figure><ul id="1555c1ac-7ebe-41b8-bd6a-6ea8bc2904da" class="bulleted-list"><li style="list-style-type:disc">최종 classifier 이전에 averge pooling layer 사용 → global average pooling이 적용된 것으로 이전 layer에서 추출된 feature map을 각각 평균 낸 것을 이어 1차원 벡터로 만들어줌; 1차원 벡터로 만들어줘야 최종적으로 이미지  분류를 위한 softmax layer와 연결할 수 있기 때문</li></ul><figure id="9f004d58-d276-42eb-9a1a-7adc3c432c74" class="image"><img style="width:624px" src="https://prod-files-secure.s3.us-west-2.amazonaws.com/366ee882-adac-4ea9-b8e2-1e685459a8a9/6562f350-d824-4f58-8814-f9262df691da/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45HZZMZUHI%2F20240728%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20240728T122023Z&amp;X-Amz-Expires=86400&amp;X-Amz-Signature=0ff768fc5dde7e27e2c045ce14c0b2a63a737a941909f3f5b5203507119003d6&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject"/></figure><ul id="fec91a8e-ff73-4423-9e0a-969250d5c85c" class="bulleted-list"><li style="list-style-type:disc">1차원 벡터로 만들면 가중치의 개수를 많이 줄여주는데, FC 방식을 이용하면 가중치 개수가  7x7x1024x1024=51.3M이지만 GAP를 이용하면 단 1개의 가중치도 필요하지 않음 → fine tuning을 하기 쉽다</li></ul><p id="c42e9b53-f063-4ab4-be63-b80a3da7d06b" class="">
</p><h3 id="594fbc56-367b-4770-8227-bce9cf3b1c11" class="">Training Methodology</h3><ul id="72c7c4bc-0540-4a14-b5da-559b5d491f0d" class="bulleted-list"><li style="list-style-type:disc">모델 훈련 방법에 대한 설명</li></ul><ul id="d69e528d-19e4-4d1d-921c-6e44d7954336" class="bulleted-list"><li style="list-style-type:disc">0.9 momentum의 stochastic gradient descent를 이용하였고, learning rate는 8 epochs마다 4% 감소시킴</li></ul><ul id="3e6395f7-85af-441b-9c87-dc8e219a945d" class="bulleted-list"><li style="list-style-type:disc">이미지의 가로:세로 비율을 3:4와 4:3 사이로 유지하며 본래 사이즈의 8~100%가 포함되도록 다양한 크기의 patch 사용</li></ul><ul id="700ea635-1b6a-4c8d-be26-4ff43127da41" class="bulleted-list"><li style="list-style-type:disc">photometric distortions를 통해 학습 데이터를 늘림 → overfitting 방지</li></ul><p id="e7468b43-917b-4c18-bce1-27875e1de36e" class="">
</p><h3 id="e6e82649-2561-48b6-b728-0b84dfa1cb77" class="">ILSVRC 2014 Classification Challenge Setup and Results</h3><div id="74cd4146-741a-4c21-aa51-05287c531622" class="column-list"><div id="2a92d3f0-3cff-44d0-99f4-b7cdde5f87c4" style="width:43.75%" class="column"><figure id="0651835f-2b19-4274-8b62-a55e784d1b95" class="image"><img style="width:593px" src="https://prod-files-secure.s3.us-west-2.amazonaws.com/366ee882-adac-4ea9-b8e2-1e685459a8a9/15183d31-f93f-4f15-9ae9-e4a2c88c5e05/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45HZZMZUHI%2F20240728%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20240728T122023Z&amp;X-Amz-Expires=86400&amp;X-Amz-Signature=b0129896c8a95d7d954ddc30480057443160dcc78f75d59f37c74d83cc1ca500&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject"/></figure></div><div id="1eab033e-0e4a-4f3c-92e9-6ee64cfc1c7f" style="width:56.25%" class="column"><figure id="546c5027-9497-4878-afdd-1889057927e8" class="image"><img style="width:725px" src="https://prod-files-secure.s3.us-west-2.amazonaws.com/366ee882-adac-4ea9-b8e2-1e685459a8a9/e876eba0-fa11-462d-9221-4ba504391ec4/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45HZZMZUHI%2F20240728%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20240728T122023Z&amp;X-Amz-Expires=86400&amp;X-Amz-Signature=ca3af6f85f49d43a8f64589c0c995b0e19464b80ecff0eedb99ab10126c5d52f&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject"/></figure></div></div><ul id="449cb20b-6aa7-4572-b351-d3045313fd98" class="bulleted-list"><li style="list-style-type:disc">2014년 ILSVRC 대회 우승을 하였고, 검증 및 테스트 데이터 모두에서 6.67%의 오차율을 기록함</li></ul><p id="a946ed56-5ae1-4176-85ba-277af4a16cbd" class="">
</p><h3 id="5240ec79-1cce-4930-9c98-507b9e06d8cf" class="">ILSVRC 2014 Detection Challenge Setup and Results</h3><div id="b5897e13-a360-4e6e-99d8-a4a19aa02102" class="column-list"><div id="05294dbb-bbe6-4dea-ae74-b7fe89432ced" style="width:56.25%" class="column"><figure id="d9c0dcd5-9791-4d3c-accb-aa802877762c" class="image"><img style="width:782px" src="https://prod-files-secure.s3.us-west-2.amazonaws.com/366ee882-adac-4ea9-b8e2-1e685459a8a9/e36b8c01-73ab-41fe-8266-4ee0dbb64e8f/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45HZZMZUHI%2F20240728%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20240728T122023Z&amp;X-Amz-Expires=86400&amp;X-Amz-Signature=5ecb22d44df321d3ca8fc850e8bdc52d6ae1e91a431b1e0ab970fe957ac01add&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject"/></figure></div><div id="3f50c3fe-8ffd-4e4a-b965-f8b5692ca574" style="width:43.75000000000001%" class="column"><figure id="3fe62944-cb4e-4c60-8011-d0d8ed17b4a7" class="image"><img style="width:704px" src="https://prod-files-secure.s3.us-west-2.amazonaws.com/366ee882-adac-4ea9-b8e2-1e685459a8a9/b9fc86af-8e2e-4246-9719-0a6cd87646a8/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45HZZMZUHI%2F20240728%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20240728T122023Z&amp;X-Amz-Expires=86400&amp;X-Amz-Signature=5d3105085320bbbe5e2f79c10330584b8e304b719fc4b223e9549f087ea1e9c1&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject"/></figure></div></div><ul id="cf01b2a8-0a3f-415e-9a64-24582451a5d4" class="bulleted-list"><li style="list-style-type:disc">Table 5에서 단일 모델만을 사용한 결과를 비교하는데, 이때 최고 성능의 모델은 Deep Insight이지만, 앙상블을 사용하게 되면 GoogLeNet은 놀라운 성능 향상을 보임.</li></ul><p id="884a231f-d221-40f6-a67f-31ffb828f9c1" class="">
</p><h3 id="9b73e996-21a8-4a46-87f4-5d0170a3561b" class=""><strong>Conclusions</strong></h3><ul id="f6f7f4da-b460-4db3-8913-ca761cd3dbfc" class="bulleted-list"><li style="list-style-type:disc">Inception 구조는 sparse 구조를 dense 구조로 근사화하여 성능을 개선함</li></ul><ul id="13b7661f-b935-4c3e-951a-a8e54fa9d710" class="bulleted-list"><li style="list-style-type:disc">기존에 CNN 성능을 높이기 위한 방법과는 다른 새로운 방법이었으며, 성능은 많이 상승하지만 연산량은 약간만 증가한다는 장점이 있음.</li></ul><p id="74f8f23b-af67-4c17-8ff5-1509b2cc61bc" class="">
</p></div></article></body></html>